name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 02:00 UTC to catch dependency issues
    - cron: '0 2 * * *'

env:
  PYTHON_DEFAULT_VERSION: "3.10"
  POETRY_VERSION: "1.6.1"
  CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

jobs:
  # Code quality and security checks
  quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better security scanning
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Cache pre-commit hooks
      uses: actions/cache@v3
      with:
        path: ~/.cache/pre-commit
        key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}
    
    - name: Run pre-commit hooks
      run: pre-commit run --all-files
    
    - name: Lint with flake8
      run: |
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
    
    - name: Lint with ruff
      run: ruff check src/ tests/
    
    - name: Type check with mypy
      run: mypy src/
    
    - name: Security scan with bandit
      run: bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Security check with safety
      run: safety check --json --output safety-report.json
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
        retention-days: 30

  # Multi-platform testing matrix
  test:
    name: Test (Python ${{ matrix.python-version }}, ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    needs: quality
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.9", "3.10", "3.11"]
        # Reduce matrix size for non-main branches
        exclude:
          - python-version: "3.9"
            os: windows-latest
          - python-version: "3.9"
            os: macos-latest
          - python-version: "3.11"
            os: windows-latest
          - python-version: "3.11"
            os: macos-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y libssl-dev libffi-dev
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run fast tests
      run: |
        pytest tests/ -v \
          --cov=src \
          --cov-report=xml \
          --cov-report=term-missing \
          --junitxml=pytest-results.xml \
          -m "not slow" \
          --maxfail=5
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          pytest-results.xml
          coverage.xml
        retention-days: 30
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == env.PYTHON_DEFAULT_VERSION
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  # Privacy and ML-specific tests
  privacy-tests:
    name: Privacy & ML Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: quality
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,benchmark]"
    
    - name: Run privacy-specific tests
      run: |
        pytest tests/ -v \
          -m "privacy" \
          --cov=src \
          --cov-report=xml \
          --cov-report=term-missing \
          --junitxml=privacy-test-results.xml
    
    - name: Run federated learning tests
      run: |
        pytest tests/ -v \
          -m "federated" \
          --cov=src \
          --cov-append \
          --cov-report=xml \
          --cov-report=term-missing \
          --junitxml=federated-test-results.xml
    
    - name: Run integration tests
      run: |
        pytest tests/ -v \
          -m "integration" \
          --cov=src \
          --cov-append \
          --cov-report=xml \
          --cov-report=term-missing \
          --junitxml=integration-test-results.xml \
          --timeout=300
    
    - name: Upload privacy test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: privacy-test-results
        path: |
          privacy-test-results.xml
          federated-test-results.xml
          integration-test-results.xml
          coverage.xml
        retention-days: 30

  # Documentation build
  docs:
    name: Documentation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: quality
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[docs]"
    
    - name: Build documentation
      run: |
        sphinx-build -b html docs/ docs/_build/html -W --keep-going
    
    - name: Upload documentation
      uses: actions/upload-artifact@v3
      with:
        name: documentation
        path: docs/_build/html
        retention-days: 30

  # Build and test Docker image
  docker:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [test, privacy-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: dp-federated-lora:ci
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64
    
    - name: Test Docker image
      run: |
        docker run --rm dp-federated-lora:ci python -c "import dp_federated_lora; print('Import successful')"
        docker run --rm dp-federated-lora:ci python -m pytest tests/ -v -m "not slow and not integration" --maxfail=3

  # Performance benchmarks (only on main branch)
  benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [test, privacy-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,benchmark]"
    
    - name: Run benchmarks
      run: |
        python -m dp_federated_lora.benchmarks.run_all \
          --output benchmarks-results.json \
          --timeout 3600
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results
        path: benchmarks-results.json
        retention-days: 90

  # Collect and report results
  results:
    name: Collect Results
    runs-on: ubuntu-latest
    needs: [quality, test, privacy-tests, docs, docker]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Display structure
      run: find . -type f -name "*.xml" -o -name "*.json" | head -20
    
    - name: Set job status
      run: |
        if [[ "${{ needs.quality.result }}" == "failure" || \
              "${{ needs.test.result }}" == "failure" || \
              "${{ needs.privacy-tests.result }}" == "failure" || \
              "${{ needs.docs.result }}" == "failure" || \
              "${{ needs.docker.result }}" == "failure" ]]; then
          echo "❌ CI Pipeline failed"
          exit 1
        else
          echo "✅ CI Pipeline completed successfully"
        fi