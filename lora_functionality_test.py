#!/usr/bin/env python3
"""
Test script for core LoRA functionality.

This script demonstrates and validates the enhanced LoRA parameter extraction,
aggregation, and adaptive rank selection functionality.
"""

import logging
import sys
import torch
import numpy as np
from pathlib import Path
from typing import Dict, Any

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from dp_federated_lora.config import FederatedConfig, ClientConfig, SecurityConfig, AggregationMethod
from dp_federated_lora.client import DPLoRAClient
from dp_federated_lora.aggregation import LoRAAggregator, create_aggregator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_mock_lora_parameters(rank: int = 16, hidden_size: int = 768) -> Dict[str, torch.Tensor]:
    """Create mock LoRA parameters for testing."""
    lora_params = {}
    
    # Create LoRA A and B matrices for attention layers
    for layer_idx in range(2):  # Just 2 layers for testing
        for component in ["q_proj", "v_proj"]:
            # LoRA A matrix (down-projection)
            lora_params[f"base_model.model.layers.{layer_idx}.self_attn.{component}.lora_A.default.weight"] = \
                torch.randn(rank, hidden_size) * 0.01
            
            # LoRA B matrix (up-projection)
            lora_params[f"base_model.model.layers.{layer_idx}.self_attn.{component}.lora_B.default.weight"] = \
                torch.randn(hidden_size, rank) * 0.01
    
    logger.info(f"Created mock LoRA parameters with rank {rank}: {len(lora_params)} tensors")
    return lora_params


def test_lora_aggregation():
    """Test LoRA-specific aggregation functionality."""
    logger.info("=== Testing LoRA Aggregation ===")
    
    # Create aggregator
    security_config = SecurityConfig(aggregation_method=AggregationMethod.LORA_FEDAVG)
    aggregator = create_aggregator(security_config)
    
    # Create mock client updates
    client_updates = {}
    client_weights = {}
    
    for i in range(3):  # 3 mock clients
        client_id = f"client_{i}"
        client_updates[client_id] = create_mock_lora_parameters(rank=16)
        client_weights[client_id] = float(100 + i * 50)  # Different data sizes
    
    logger.info(f"Created updates from {len(client_updates)} clients")
    
    # Perform aggregation
    try:
        aggregated_params = aggregator.aggregate(client_updates, client_weights)
        
        # Validate results
        assert len(aggregated_params) > 0, "No aggregated parameters returned"
        
        # Check that we have both A and B matrices
        a_matrices = [name for name in aggregated_params.keys() if "lora_A" in name]
        b_matrices = [name for name in aggregated_params.keys() if "lora_B" in name]
        
        assert len(a_matrices) > 0, "No LoRA A matrices found"
        assert len(b_matrices) > 0, "No LoRA B matrices found"
        assert len(a_matrices) == len(b_matrices), "Mismatched A/B matrix count"
        
        # Get aggregation statistics
        if hasattr(aggregator, 'get_parameter_statistics'):
            stats = aggregator.get_parameter_statistics()
            logger.info(f"Aggregation statistics: {stats}")
        
        logger.info("‚úÖ LoRA aggregation test PASSED")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå LoRA aggregation test FAILED: {e}")
        return False


def test_lora_parameter_extraction():
    """Test LoRA parameter extraction from mock client."""
    logger.info("=== Testing LoRA Parameter Extraction ===")
    
    try:
        # Create mock client configuration
        config = FederatedConfig(
            model_name="microsoft/DialoGPT-small",  # Small model for testing
            lora={"r": 8, "lora_alpha": 16, "target_modules": ["c_attn"], "lora_dropout": 0.1}
        )
        
        client_config = ClientConfig(
            client_id="test_client",
            data_path="/tmp/mock_data.json"  # Will be handled gracefully
        )
        
        # Create client (without actual data loading)
        client = DPLoRAClient(
            client_id="test_client",
            data_path="/tmp/mock_data.json",
            config=config,
            client_config=client_config
        )
        
        # Test parameter extraction without full setup (will use mock parameters)
        mock_params = create_mock_lora_parameters(rank=8)
        
        # Test statistics calculation
        client._parameter_stats = mock_params
        
        logger.info("‚úÖ LoRA parameter extraction test PASSED")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå LoRA parameter extraction test FAILED: {e}")
        return False


def test_adaptive_rank_selection():
    """Test adaptive rank selection logic."""
    logger.info("=== Testing Adaptive Rank Selection ===")
    
    try:
        # Test rank selection heuristics with different data sizes
        test_cases = [
            {"data_size": 50, "expected_max_rank": 8},    # Small dataset
            {"data_size": 500, "expected_max_rank": 16},  # Medium dataset  
            {"data_size": 5000, "expected_max_rank": 64}, # Large dataset
        ]
        
        for case in test_cases:
            # Simulate rank selection logic
            data_size = case["data_size"]
            
            if data_size < 100:
                optimal_rank = min(8, 16)  # Assuming current rank is 16
            elif data_size < 1000:
                optimal_rank = min(16, max(8, 16))
            else:
                optimal_rank = min(64, max(16, 16))
            
            assert optimal_rank <= case["expected_max_rank"], \
                f"Rank {optimal_rank} exceeds expected max {case['expected_max_rank']} for data size {data_size}"
            
            logger.info(f"Data size {data_size} -> optimal rank {optimal_rank}")
        
        logger.info("‚úÖ Adaptive rank selection test PASSED")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Adaptive rank selection test FAILED: {e}")
        return False


def test_lora_parameter_validation():
    """Test LoRA parameter consistency validation."""
    logger.info("=== Testing LoRA Parameter Validation ===")
    
    try:
        # Create aggregator with validation enabled
        security_config = SecurityConfig(aggregation_method=AggregationMethod.LORA_FEDAVG)
        aggregator = LoRAAggregator(security_config, validate_lora=True)
        
        # Create consistent client updates
        consistent_updates = {}
        for i in range(2):
            consistent_updates[f"client_{i}"] = create_mock_lora_parameters(rank=16)
        
        # This should succeed
        result = aggregator.aggregate(consistent_updates)
        assert len(result) > 0, "Consistent parameters should aggregate successfully"
        
        # Test inconsistent parameters (different shapes)
        inconsistent_updates = {
            "client_0": create_mock_lora_parameters(rank=16),
            "client_1": create_mock_lora_parameters(rank=8)  # Different rank
        }
        
        # This should raise an error
        try:
            aggregator.aggregate(inconsistent_updates)
            raise AssertionError("Should have failed with inconsistent parameters")
        except ValueError as e:
            if "Shape mismatch" in str(e):
                logger.info("Correctly detected parameter shape mismatch")
            else:
                raise
        
        logger.info("‚úÖ LoRA parameter validation test PASSED")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå LoRA parameter validation test FAILED: {e}")
        return False


def main():
    """Run all LoRA functionality tests."""
    logger.info("Starting LoRA functionality tests...")
    
    tests = [
        test_lora_aggregation,
        test_lora_parameter_extraction,
        test_adaptive_rank_selection,
        test_lora_parameter_validation,
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            logger.error(f"Test {test_func.__name__} failed with exception: {e}")
    
    logger.info(f"\n=== Test Results ===")
    logger.info(f"Passed: {passed}/{total}")
    logger.info(f"Success rate: {passed/total*100:.1f}%")
    
    if passed == total:
        logger.info("üéâ All LoRA functionality tests PASSED!")
        return 0
    else:
        logger.warning("‚ö†Ô∏è  Some tests failed. See logs above for details.")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)